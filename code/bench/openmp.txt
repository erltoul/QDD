This file summarizes first benchmarks with OpenMP in the 3D code.
Besides OpenMP, the code now exploits the 3D versions of FFTW3 in the
exact Coulomb solver and in the fast propagator 'kinprop'.  The
following benchmarks show computing time per dynamic step (measured in
'tstep') in seconds. The CPU time seems to accumulate consumption in
case of multi-threading. The CLOCK time measures the actual time
needed. The cases are named almost self explanatory.  
Still, a few words are in order: 
 * Standard case is 'FFTW' which means using 3D routines of FFTW3 
   in exact Coulomb solver and kinetic propagation 'kinprop'
   (activated, of course, by 'iffastpropag=1'). Only deviations
   from this standard are announced.
 * 'old kinprop' employs the old coding of interlaced 1D FFT's
   in 'kinprop'.
 * 'old Coulex' employs the old coding of interlaced 1D FFT's
   in the exact Coulomb solver.
 * 'OMPn' means OpenMP with n nodes.
 * 'OMPn parallele FFT' means employing the OpenMP implementation 
   of FFTW3.
 * 'OMPn parallele dyn' means to compute kinetic propagation of
   each wavefunction in parallele plus employing the OpenMP 
   implementation of FFTW3 in the exact Coulomb solver.
All options can be run with the code as it resides now on the
branch 'dev-FFTW-omp'. Activation and compilation is explained
in 'doc/openmp_explain.txt'.


Test case Na_40, jellium, no SIC, 64**3 grid
--------------------------------------------

                                                    
                          CPU(cumul.)  CLOCK      %gain 
FFTW_EXHAUSTIVE
NETLIB                         0.99    0.99
FFTW iffastpropag=0            0.80    0.80
FFTW old kinprop (1D fft's)    0.79    0.79
FFTW old Coulex (1D fft's)     0.80    0.80
FFTW                           0.71    0.71         0
OMP2 parallele FFTW            0.74    0.50        30
OMP2 parallele wfs             0.99    0.49        31
OMP4 parallele FFTW            0.84    0.40        44
OMP2 parallele wfs             1.19    0.40        44

FFTW_MEASURE
FFTW                           0.71    0.71
OMP2 parallele FFTW            0.77    0.51



Test case Na_40, jellium, no SIC, 128**3 grid
---------------------------------------------

                          CPU(cumul.)  CLOCK      %gain 
FFTW_EXHAUSTIVE
NETLIB                          8.8     8.8
FFTW iffastpropag=0             8.3     8.3
FFTW old kinprop (1D fft's)     7.6     7.6
FFTW old Coulex (1D fft's)      8.2     8.2
FFTW                            7.5     7.5         0
OMP2 parallele FFTW             7.5     4.8        36
OMP2 parallele wfs              8.3     4.7        37
OMP4 parallele FFTW             8.5     3.7        51
OMP2 parallele wfs             10.0     3.5        53

FFTW_MEASURE
FFTW                            7.5     7.5
OMP2 parallele FFTW             8.4     5.3


Comments and observations:
---------------------------

1) The new installation employing 3D FFT's from FFTW3 delivers
   another small gain in speed than the options using interlaced 
   1D FFT's. Taken all gains together and comparing now to the old 
   NETLIB installation, we come now to gratifying total gains or 
   order of 15-28 %. 

2) OpenMP with 2 nodes yields 30-37% net gain in speed. Going up
   to 4 nodes yields 44-51% gain. This latter step is not so
   efficient because we still have a constant background of
   single node processing. 

3) Wavefunction parallelization and FFTW3 parallelization delivers
   comparable gains. But the FFTW3 parallelization is simpler in
   coding and requires less workspace. It may be the preferable
   option.

4) Gains by parallelization are larger for the large grid. This is
   good news as large grids are the cases where we aim at more
   speed.

5) We still should find out the one-node pieces which inhibit 
   better parallele performance. The known next time eaters are
   the LDA computuation and the copying operations in 'kinprop'
   and in the Coulomb solver. We should wrk on these.
